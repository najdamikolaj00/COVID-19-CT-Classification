{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report as report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_transformer = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop((224), scale=(0.5, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "val_transformer = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_COVID='CT_COVID.txt'\n",
    "txt_NonCOVID='CT_NonCOVID.txt'\n",
    "\n",
    "data_COVID='Data/CT_COVID'\n",
    "data_NonCOVID='Data/CT_NonCOVID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_COVID = r'C:\\Users\\PC\\ML_Classification_Project\\COVID-19-CT-Classification\\Data\\CT_COVID'\n",
    "data_NonCOVID = r'C:\\Users\\PC\\ML_Classification_Project\\COVID-19-CT-Classification\\Data\\CT_NonCOVID'\n",
    "\n",
    "with open(txt_COVID, 'r') as file:\n",
    "    COVID_train_paths = [os.path.join(data_COVID, line) for line in file.read().splitlines()]\n",
    "\n",
    "with open(txt_NonCOVID, 'r') as file:\n",
    "    NonCOVID_train_paths = [os.path.join(data_NonCOVID, line) for line in file.read().splitlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID_train_labels = np.ones(len(COVID_train_paths))\n",
    "NonCOVID_train_labels = np.zeros(len(NonCOVID_train_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_paths = np.concatenate((COVID_train_paths, NonCOVID_train_paths), axis=0)\n",
    "all_labels = np.concatenate((COVID_train_labels, NonCOVID_train_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv_dataset_split(all_image_paths, all_labels, k_folds):\n",
    "    random_state = 42\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    train_splits = []\n",
    "    val_splits = []\n",
    "\n",
    "    for train_index, val_index in kfold.split(all_image_paths):\n",
    "        train_paths_fold = all_image_paths[train_index]\n",
    "        val_paths_fold = all_image_paths[val_index]\n",
    "        train_labels_fold = all_labels[train_index]\n",
    "        val_labels_fold = all_labels[val_index]\n",
    "\n",
    "        train_splits.append((train_paths_fold, train_labels_fold))\n",
    "        val_splits.append((val_paths_fold, val_labels_fold))\n",
    "\n",
    "    return train_splits, val_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(train_paths, train_labels, val_paths, val_labels):\n",
    "    # Load the images and labels for training set with transformations\n",
    "    train_images = []\n",
    "    for path in train_paths:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = train_transformer(image)  # Apply transformations\n",
    "        train_images.append(image)\n",
    "\n",
    "    # Resize images to a consistent size\n",
    "    resized_train_images = [transforms.Resize((224, 224))(image) for image in train_images]\n",
    "\n",
    "    # Convert the list of resized images to a NumPy array\n",
    "    X_train = np.array([np.array(image) for image in resized_train_images])\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    y_train = train_labels\n",
    "\n",
    "    # Load the images and labels for validation set with transformations\n",
    "    val_images = []\n",
    "    for path in val_paths:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = val_transformer(image)  # Apply transformations\n",
    "        val_images.append(image)\n",
    "\n",
    "    # Resize images to a consistent size\n",
    "    resized_val_images = [transforms.Resize((224, 224))(image) for image in val_images]\n",
    "\n",
    "    # Convert the list of resized images to a NumPy array\n",
    "    X_val = np.array([np.array(image) for image in resized_val_images])\n",
    "    X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "    y_val = val_labels\n",
    "\n",
    "    # Step 2: Perform PCA Dimensionality Reduction\n",
    "    # Apply PCA to reduce the dimensionality of the image data\n",
    "    pca = PCA(n_components=173)  # Choose the desired number of components\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "\n",
    "    # Step 3: Train the SVM Classifier\n",
    "    # Initialize an SVM classifier\n",
    "    svm_classifier = SVC()\n",
    "\n",
    "    # Train the SVM classifier on the reduced training data\n",
    "    svm_classifier.fit(X_train_pca, y_train)\n",
    "\n",
    "    # Step 4: Evaluate the Model\n",
    "    # Use the trained SVM classifier to predict the labels of the validation data\n",
    "    y_val_pred = svm_classifier.predict(X_val_pca)\n",
    "\n",
    "    # Evaluate the accuracy and other performance metrics of the model\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    classification_report_result = report(y_val, y_val_pred)\n",
    "\n",
    "    return accuracy, classification_report_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold_notPCA(train_paths, train_labels, val_paths, val_labels):\n",
    "    # Load the images and labels for training set with transformations\n",
    "    train_images = []\n",
    "    for path in train_paths:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = train_transformer(image)  # Apply transformations\n",
    "        train_images.append(image)\n",
    "\n",
    "    # Resize images to a consistent size\n",
    "    resized_train_images = [transforms.Resize((224, 224))(image) for image in train_images]\n",
    "\n",
    "    # Convert the list of resized images to a NumPy array\n",
    "    X_train = np.array([np.array(image) for image in resized_train_images])\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    y_train = train_labels\n",
    "\n",
    "    # Load the images and labels for validation set with transformations\n",
    "    val_images = []\n",
    "    for path in val_paths:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = val_transformer(image)  # Apply transformations\n",
    "        val_images.append(image)\n",
    "\n",
    "    # Resize images to a consistent size\n",
    "    resized_val_images = [transforms.Resize((224, 224))(image) for image in val_images]\n",
    "\n",
    "    # Convert the list of resized images to a NumPy array\n",
    "    X_val = np.array([np.array(image) for image in resized_val_images])\n",
    "    X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "    y_val = val_labels\n",
    "\n",
    "\n",
    "    # Step 3: Train the SVM Classifier\n",
    "    # Initialize an SVM classifier\n",
    "    svm_classifier = SVC()\n",
    "\n",
    "    # Train the SVM classifier on the reduced training data\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Step 4: Evaluate the Model\n",
    "    # Use the trained SVM classifier to predict the labels of the validation data\n",
    "    y_val_pred = svm_classifier.predict(X_val)\n",
    "\n",
    "    # Evaluate the accuracy and other performance metrics of the model\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    classification_report_result = report(y_val, y_val_pred)\n",
    "\n",
    "    return accuracy, classification_report_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "train_splits, val_splits = k_fold_cv_dataset_split(all_image_paths, all_labels, num_folds)\n",
    "\n",
    "fold_accuracies = []\n",
    "fold_classification_reports = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m val_paths_fold, val_labels_fold \u001b[39m=\u001b[39m val_split\n\u001b[0;32m      7\u001b[0m \u001b[39m# Train and evaluate the model for the current fold\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m accuracy, classification_report_result \u001b[39m=\u001b[39m train_fold(train_paths_fold, train_labels_fold, val_paths_fold, val_labels_fold)\n\u001b[0;32m     10\u001b[0m fold_accuracies\u001b[39m.\u001b[39mappend(accuracy)\n\u001b[0;32m     11\u001b[0m fold_classification_reports\u001b[39m.\u001b[39mappend(classification_report_result)\n",
      "Cell \u001b[1;32mIn[10], line 35\u001b[0m, in \u001b[0;36mtrain_fold\u001b[1;34m(train_paths, train_labels, val_paths, val_labels)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m# Step 2: Perform PCA Dimensionality Reduction\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m# Apply PCA to reduce the dimensionality of the image data\u001b[39;00m\n\u001b[0;32m     34\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m173\u001b[39m)  \u001b[39m# Choose the desired number of components\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m X_train_pca \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[0;32m     36\u001b[0m X_val_pca \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mtransform(X_val)\n\u001b[0;32m     38\u001b[0m \u001b[39m# Step 3: Train the SVM Classifier\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m# Initialize an SVM classifier\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\ML_Classification_Project\\COVID-19-CT-Classification\\env\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\PC\\ML_Classification_Project\\COVID-19-CT-Classification\\env\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:462\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mC-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 462\u001b[0m U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m    463\u001b[0m U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\ML_Classification_Project\\COVID-19-CT-Classification\\env\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:514\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_full(X, n_components)\n\u001b[0;32m    513\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m--> 514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_truncated(X, n_components, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_svd_solver)\n",
      "File \u001b[1;32mc:\\Users\\PC\\ML_Classification_Project\\COVID-19-CT-Classification\\env\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:618\u001b[0m, in \u001b[0;36mPCA._fit_truncated\u001b[1;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[0;32m    614\u001b[0m     U, Vt \u001b[39m=\u001b[39m svd_flip(U[:, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], Vt[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m    616\u001b[0m \u001b[39melif\u001b[39;00m svd_solver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    617\u001b[0m     \u001b[39m# sign flipping is done inside\u001b[39;00m\n\u001b[1;32m--> 618\u001b[0m     U, S, Vt \u001b[39m=\u001b[39m randomized_svd(\n\u001b[0;32m    619\u001b[0m         X,\n\u001b[0;32m    620\u001b[0m         n_components\u001b[39m=\u001b[39;49mn_components,\n\u001b[0;32m    621\u001b[0m         n_oversamples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_oversamples,\n\u001b[0;32m    622\u001b[0m         n_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterated_power,\n\u001b[0;32m    623\u001b[0m         power_iteration_normalizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpower_iteration_normalizer,\n\u001b[0;32m    624\u001b[0m         flip_sign\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    625\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m    626\u001b[0m     )\n\u001b[0;32m    628\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples_ \u001b[39m=\u001b[39m n_samples\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents_ \u001b[39m=\u001b[39m Vt\n",
      "File \u001b[1;32mc:\\Users\\PC\\ML_Classification_Project\\COVID-19-CT-Classification\\env\\lib\\site-packages\\sklearn\\utils\\extmath.py:446\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m transpose:\n\u001b[0;32m    443\u001b[0m     \u001b[39m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[0;32m    444\u001b[0m     M \u001b[39m=\u001b[39m M\u001b[39m.\u001b[39mT\n\u001b[1;32m--> 446\u001b[0m Q \u001b[39m=\u001b[39m randomized_range_finder(\n\u001b[0;32m    447\u001b[0m     M,\n\u001b[0;32m    448\u001b[0m     size\u001b[39m=\u001b[39;49mn_random,\n\u001b[0;32m    449\u001b[0m     n_iter\u001b[39m=\u001b[39;49mn_iter,\n\u001b[0;32m    450\u001b[0m     power_iteration_normalizer\u001b[39m=\u001b[39;49mpower_iteration_normalizer,\n\u001b[0;32m    451\u001b[0m     random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[0;32m    452\u001b[0m )\n\u001b[0;32m    454\u001b[0m \u001b[39m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[0;32m    455\u001b[0m B \u001b[39m=\u001b[39m safe_sparse_dot(Q\u001b[39m.\u001b[39mT, M)\n",
      "File \u001b[1;32mc:\\Users\\PC\\ML_Classification_Project\\COVID-19-CT-Classification\\env\\lib\\site-packages\\sklearn\\utils\\extmath.py:275\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39melif\u001b[39;00m power_iteration_normalizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLU\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    274\u001b[0m     Q, _ \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39mlu(safe_sparse_dot(A, Q), permute_l\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 275\u001b[0m     Q, _ \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39mlu(safe_sparse_dot(A\u001b[39m.\u001b[39;49mT, Q), permute_l\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    276\u001b[0m \u001b[39melif\u001b[39;00m power_iteration_normalizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mQR\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    277\u001b[0m     Q, _ \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39mqr(safe_sparse_dot(A, Q), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meconomic\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\ML_Classification_Project\\COVID-19-CT-Classification\\env\\lib\\site-packages\\sklearn\\utils\\extmath.py:189\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[0;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    192\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[0;32m    193\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[0;32m    194\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[0;32m    195\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m ):\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_split, val_split) in enumerate(zip(train_splits, val_splits)):\n",
    "    print(\"Fold:\", fold + 1)\n",
    "\n",
    "    train_paths_fold, train_labels_fold = train_split\n",
    "    val_paths_fold, val_labels_fold = val_split\n",
    "\n",
    "    # Train and evaluate the model for the current fold\n",
    "    accuracy, classification_report_result = train_fold(train_paths_fold, train_labels_fold, val_paths_fold, val_labels_fold)\n",
    "\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_classification_reports.append(classification_report_result)\n",
    "\n",
    "    print(\"Validation Set for Fold\", fold + 1)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report_result)\n",
    "\n",
    "avg_accuracy = np.mean(fold_accuracies)\n",
    "print(\"Average Accuracy:\", avg_accuracy)\n",
    "\n",
    "# Print the classification report for each fold\n",
    "for i, classification_report_result in enumerate(fold_classification_reports):\n",
    "    print(\"\\nClassification Report for Fold\", i+1)\n",
    "    print(classification_report_result)\n",
    "\n",
    "# Aggregate the classification reports across all folds\n",
    "combined_classification_report = \"\\n\".join(fold_classification_reports)\n",
    "print(\"\\nCombined Classification Report:\")\n",
    "print(combined_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Validation Set for Fold 1\n",
      "Accuracy: 0.6866666666666666\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.80      0.73        80\n",
      "         1.0       0.71      0.56      0.62        70\n",
      "\n",
      "    accuracy                           0.69       150\n",
      "   macro avg       0.69      0.68      0.68       150\n",
      "weighted avg       0.69      0.69      0.68       150\n",
      "\n",
      "Fold: 2\n",
      "Validation Set for Fold 2\n",
      "Accuracy: 0.6912751677852349\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.87      0.74        75\n",
      "         1.0       0.79      0.51      0.62        74\n",
      "\n",
      "    accuracy                           0.69       149\n",
      "   macro avg       0.72      0.69      0.68       149\n",
      "weighted avg       0.72      0.69      0.68       149\n",
      "\n",
      "Fold: 3\n",
      "Validation Set for Fold 3\n",
      "Accuracy: 0.6375838926174496\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.74      0.67        74\n",
      "         1.0       0.68      0.53      0.60        75\n",
      "\n",
      "    accuracy                           0.64       149\n",
      "   macro avg       0.64      0.64      0.63       149\n",
      "weighted avg       0.64      0.64      0.63       149\n",
      "\n",
      "Fold: 4\n",
      "Validation Set for Fold 4\n",
      "Accuracy: 0.7248322147651006\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.82      0.78        88\n",
      "         1.0       0.69      0.59      0.64        61\n",
      "\n",
      "    accuracy                           0.72       149\n",
      "   macro avg       0.72      0.70      0.71       149\n",
      "weighted avg       0.72      0.72      0.72       149\n",
      "\n",
      "Fold: 5\n",
      "Validation Set for Fold 5\n",
      "Accuracy: 0.7516778523489933\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.82      0.78        80\n",
      "         1.0       0.77      0.67      0.71        69\n",
      "\n",
      "    accuracy                           0.75       149\n",
      "   macro avg       0.75      0.75      0.75       149\n",
      "weighted avg       0.75      0.75      0.75       149\n",
      "\n",
      "Average Accuracy: 0.6984071588366889\n",
      "\n",
      "Classification Report for Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.80      0.73        80\n",
      "         1.0       0.71      0.56      0.62        70\n",
      "\n",
      "    accuracy                           0.69       150\n",
      "   macro avg       0.69      0.68      0.68       150\n",
      "weighted avg       0.69      0.69      0.68       150\n",
      "\n",
      "\n",
      "Classification Report for Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.87      0.74        75\n",
      "         1.0       0.79      0.51      0.62        74\n",
      "\n",
      "    accuracy                           0.69       149\n",
      "   macro avg       0.72      0.69      0.68       149\n",
      "weighted avg       0.72      0.69      0.68       149\n",
      "\n",
      "\n",
      "Classification Report for Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.74      0.67        74\n",
      "         1.0       0.68      0.53      0.60        75\n",
      "\n",
      "    accuracy                           0.64       149\n",
      "   macro avg       0.64      0.64      0.63       149\n",
      "weighted avg       0.64      0.64      0.63       149\n",
      "\n",
      "\n",
      "Classification Report for Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.82      0.78        88\n",
      "         1.0       0.69      0.59      0.64        61\n",
      "\n",
      "    accuracy                           0.72       149\n",
      "   macro avg       0.72      0.70      0.71       149\n",
      "weighted avg       0.72      0.72      0.72       149\n",
      "\n",
      "\n",
      "Classification Report for Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.82      0.78        80\n",
      "         1.0       0.77      0.67      0.71        69\n",
      "\n",
      "    accuracy                           0.75       149\n",
      "   macro avg       0.75      0.75      0.75       149\n",
      "weighted avg       0.75      0.75      0.75       149\n",
      "\n",
      "\n",
      "Combined Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.80      0.73        80\n",
      "         1.0       0.71      0.56      0.62        70\n",
      "\n",
      "    accuracy                           0.69       150\n",
      "   macro avg       0.69      0.68      0.68       150\n",
      "weighted avg       0.69      0.69      0.68       150\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.87      0.74        75\n",
      "         1.0       0.79      0.51      0.62        74\n",
      "\n",
      "    accuracy                           0.69       149\n",
      "   macro avg       0.72      0.69      0.68       149\n",
      "weighted avg       0.72      0.69      0.68       149\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.74      0.67        74\n",
      "         1.0       0.68      0.53      0.60        75\n",
      "\n",
      "    accuracy                           0.64       149\n",
      "   macro avg       0.64      0.64      0.63       149\n",
      "weighted avg       0.64      0.64      0.63       149\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.82      0.78        88\n",
      "         1.0       0.69      0.59      0.64        61\n",
      "\n",
      "    accuracy                           0.72       149\n",
      "   macro avg       0.72      0.70      0.71       149\n",
      "weighted avg       0.72      0.72      0.72       149\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.82      0.78        80\n",
      "         1.0       0.77      0.67      0.71        69\n",
      "\n",
      "    accuracy                           0.75       149\n",
      "   macro avg       0.75      0.75      0.75       149\n",
      "weighted avg       0.75      0.75      0.75       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_split, val_split) in enumerate(zip(train_splits, val_splits)):\n",
    "    print(\"Fold:\", fold + 1)\n",
    "\n",
    "    train_paths_fold, train_labels_fold = train_split\n",
    "    val_paths_fold, val_labels_fold = val_split\n",
    "\n",
    "    # Train and evaluate the model for the current fold\n",
    "    accuracy, classification_report_result = train_fold_notPCA(train_paths_fold, train_labels_fold, val_paths_fold, val_labels_fold)\n",
    "\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_classification_reports.append(classification_report_result)\n",
    "\n",
    "    print(\"Validation Set for Fold\", fold + 1)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_report_result)\n",
    "\n",
    "avg_accuracy = np.mean(fold_accuracies)\n",
    "print(\"Average Accuracy:\", avg_accuracy)\n",
    "\n",
    "# Print the classification report for each fold\n",
    "for i, classification_report_result in enumerate(fold_classification_reports):\n",
    "    print(\"\\nClassification Report for Fold\", i+1)\n",
    "    print(classification_report_result)\n",
    "\n",
    "# Aggregate the classification reports across all folds\n",
    "combined_classification_report = \"\\n\".join(fold_classification_reports)\n",
    "print(\"\\nCombined Classification Report:\")\n",
    "print(combined_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
